---
layout: static1
title: Saving Time With User-Centered Design
permalink: /cw-assessment-full
---
<h2>The Challenge</h2>
Each year, the Cadet Wing (CW) assesses the over 4000 cadets at the US Air Force Academy. Two components of this&mdash;the personal appearance inspection and cadet interview&mdash;had been carried out on paper or used a survey tool to log interview responses. To compile and analyze the data at the end of the assessment period required hundreds of hours of work in addition to the large number of hours spent carrying out the inspections and interviews. With the recent acquisition of a Learning Management System (LMS), the CW wanted to utilize the LMS to save time and have all the data entered and stored in one location. They contacted our team, and I took the lead on this project, carrying out the design and development of the products used to collect the assessment data, as well as the training on how to use them. 
<br>
<h2>The Iterative Design Process</h2>
<p>At our kickoff meeting, the CW leadership stated their goals for this project and described how the inspection and interview took place. Given the nature of the project, I determined that iterative design was the best way to arrive at the optimal products. So, we agreed to meet at regular intervals so I could show them working prototypes and conduct usability testing in order to gather feedback to improve the design of the products.</p>

<h3>User Research</h3>
<p>During our initial meeting we also discussed who would be using the products. <font class="text-primary">Our team was unable to conduct our own user research, so we relied on demographics provided by the leadership</font>, some of whom would also be assessors. The leadership informed us that <u>the assessors</u>:
  <ul>
    <li> Are time crunched: they do not have much time to learn a new tool or tools, and assessments need to be completed efficiently.</li>
    <li> Exhibit a spectrum of comfort and abilities with technology: the product needs to be easy to use and intuitive.</li>
    <li> Feel pressure to have their squadron receive high marks.</li>
</ul>
While the assessors woud be the primary users, I recognized that those handling the data after the assessment would be users as well (even if they didn't see themselves as such). Because some of the data handlers were those in leadership roles, I was able to informally interview them to find out their needs. From our interactions, I gathered that <u>those analyzing the data</u>: 
  <ul>
    <li> Wanted granularity in the data: they needed to be able to track comments, which interview questions were asked, and how many points were given for each category.</li>
    <li> Were concerned about the integrity of the data.</li>
    <li> Needed one-stop access to data: data for both assessments needs to be stored in one location.</li>
    <li> Had more familiarity and comfort with technology than most assessors.</li>
</ul>
</p>

<h3>Initial Prototypes: Choosing the Best Tool for the Job</h3>
<p>With this information about the two user groups, I developed initial prototypes using two different tools in the LMS: rubrics and tests. During my second meeting, my goal was to come to a decision on which tool to use for each component of the assessment. At this meeting I used low-fidelity prototypes to demonstrate the capabilities of each tool, noting how they would impact the user experience with respect to each user group's tasks: inputting data, and collecting and analyzing data.</p>
  
By the end of this meeting <font class="text-success">we concluded that the rubric tool was the tool for both</font> the appearance inspection (a straightforward use of a rubric) and the interview (a rather innovative use of the tool). This decision hinged on <font class="text-primary">six considerations related to the users' needs</font>:
<ol>
  <li> Assessors would only need to learn and use on tool instead of two.</li>
  <li> The workflow for the assessor was simplified: the best option for the test tool would involve multiple accounts and thereby slow down the assessment process, whereas the rubric tool required a single sign-on.</li>
  <li> Data retrieval with a rubric would be much cleaner than with the test tool.</li>
  <li> It would be easy to track changes in the data, thus helping ensure its integrity.</li>
  <li> The ability to provide written feedback in addition to a numerical score.</li>
  <li> The ability to track which interview questions were asked (since the interviewer was allowed to ask one of several questions per category).</li>
</ol>
</p>
<p> Our subsequent meetings were spent working out three components of the user experience: how the assessor would select the appropriate rubric; how each rubric would calculate points; and whether to include an automatically assigned pass/fail grade after the assessment and, if so, whether to color code it.</p>
 
<h3>Desiging the Right Rubric: Prototype, Test, Repeat</h3>
<p>Once the rubric tool was chosen, we started focusing on designing the rubrics to provide an optimal user experience. To get there, we had to decide:
  <ul>
    <li> The best way to group the cadets (create one course in the LMS with all of the cadets, create one course per class year, create one class per squadron, etc).</li>
    <li> The best way to access the rubrics within a course.</li>
</ul>
In what follows, you'll see screenshots of pieces of the prototypes I developed for each meeting. <font class="text-primary">I decided to build prototypes for each meeting because they were quick and easy to build, and because it would allow me to conduct usability testing during each meeting: I could give the leadership access to a testing course so they could get a sense for how the tool would function.</font></p>

<h4>Grouping the Cadets</h4>
<p>To use the rubric for assessment, we needed to house it within a course in the LMS. Because of that, we had to decide how many courses to use. <font class="text-success">Ultimately, it made the most sense to have one course per class year, or four courses total.</font> On the data collection side, having only one course would have centralized the data more&mdash;but it would have been much more cumbersome for the assessors. And while we could also have created one course per squadron, which would have had some benefit for the assessors, everyone agreed that creating 40 courses was not worth the marginal improvement in the assessor's workflow.</p>
  
<p>With the number of courses settled, we had a clear picture of the user journey for completing each assessment.
<br><img class="img-responsive" src="/img/CW-Rubric-Journey.jpg" alt="The user journey for either rubric"><br></p>

<h4>Accessing the Rubrics</h4>
<p>Within the LMS, a rubric is associated with a column in the course’s gradebook. Any column can have more than one rubric associated with it. With the personal appearance inspection, there were two different rubrics, depending on the choice of uniform. So, one way to place the rubrics in the course was to associate both with one column, which would look like this:<br>
  <img class="img-responsive" src="/img/01-RubricSelection.PNG" alt="Choosing between the two rubrics"></p>

<p> Once they made their choice, they would see the rubric they selected, as well as a choice to use the other rubric (see top-right). After filling out the selected rubric they could submit the score or move onto the next one and submit:<br>
  <img class="img-responsive" src="/img/02-Rubric-VerticalFlow.png" alt="The first and second rubric options"><br>
One downside of this approach is that it would be easy to mistakenly fill in the wrong rubric: seeing <q>1 of 2</q> might make an assessor believe they had two rubrics to fill in instead of one.</p>

<p><font class="text-success">The leadership chose the second option</font>, which was to have two columns, one for one uniform choice and a second for the other. The rubric looked the same but no longer had <q>1 of 2</q> and the option to choose a second rubric once it was open:
  <br><img class="img-responsive" src="/img/03-Rubric.PNG" alt="The chosen rubric, with text box"><br>
(A quick note about the user interface: the LMS does not let you change the number of columns for certain rows, hence the “N/A DO NOT USE THIS BOX” boxes. If I had my druthers, I would have changed the number of options available in those rows. Also note the ability to add written feedback about a given mark.)</p>

<h3>Assigning Points</h3>
<p>One major component of this project was determining the best way to allocate points. For the interview I had to make sure that the first few rows of the rubric had no points, since they contained the questions the interviewer asked the cadets and were not worth any points. I realized that I could do this by exploiting the weighting function of the rubric, and assigning a 0% weight to those rows and non-zero weight to the rest (pretty neat, right?&mdash;which questions were asked were tracked by selecting the appropriate question within the rubric, while the rest of the rows were used for evaluating the cadet’s responses).
<br><img class="img-responsive" src="/img/04-Interview-Headers.PNG" alt="The interview rubric, with the categories in each cell."></p>

<p>Given the length of the rubric, the other consideration for the interview and personal appearance inspection was whether to include the verbal description of their assessment in each cell (look below the percentage in each cell in the following screenshots):
<br><img class="img-responsive" src="/img/04-Interview-Comparison.png" alt="The top rubric doesn't have the categories in cell whereas the bottom does."><br>
Since the rubrics would require scrolling to reach the bottom, I advised including the verbal description in each box so the assessor didn’t have to remember which column corresponded to which description. <font class="text-success">The leadership endorsed my recommendation in the final design.</font></p>

<p>For the uniform inspection rubric, there were two ways the rubric could calculate points: each category could be seen as an opportunity to gain points (i.e. a cadet started with a low score and worked his or her way to a higher score), or an opportunity to lose points (i.e. they started with a high score and lost points for infractions). <font class="text-primary">Working through the choice of point-scoring schemas provided an opportunity to consult the leadership on a heretofore unconsidered user: the cadet.</font> While the assessors and staff would be the primary users of these rubrics, the cadets would eventually see the rubrics, and it was important to think about how they would view the point distribution: should they view the inspection as an opportunity to prove themselves or to not make a mistake?
<br>
<img class="img-responsive" src="/img/06-Schema-Small1.PNG" alt="Point schema: lose points.">
<img class="img-responsive" src="/img/06-Schema-Small2.PNG" alt="Point schema: earn points.">
<br>
Between these, the leadership chose the option where cadets are trying not to lose points.</p>

<h3>Assigning a Grade: An Sneak Peak Before Analyzing the Data</h3>
<p>One feature of the LMS is that you can create a grading schema to convert a numerical score to a letter grade. Again keeping the cadet in mind, I presented this option to the CW leadership as something we could add in. Once the inspection or interview rubric was completed and saved, the LMS would show a pass or fail grade along with the score received on each assessment. The leadership liked the thought of providing a letter grade so the cadets knew whether they passed, so we implemented a grading schema based on their requirements into the final solution.</p>

<h2>Outcome</h2>
<p>Once the design process was over, I trained over 80 people who would use the rubrics and train 120 others on how to use the rubrics. 
  The feedback we received at the end of the assessment period was overwhelmingly positive. Among the positive feedback we received was a letter of commendation from the Vice Commandant of Cadets noting how <q>Dr. Padgett’s innovative placement of the Personal Appearance Inspection (PAI) into Blackboard is a game changer for us</q>. And the numbers do not lie about that: not only were the users happy with the experience provided by the rubrics, but overall <font class="text-success">these rubrics saved CW staff 915 hours of work (per assessment cycle)!</font></p>
