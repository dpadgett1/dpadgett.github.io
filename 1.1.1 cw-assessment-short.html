---
layout: static1
title: Saving Time With User-Centered Design
permalink: /cw-assessment-short
---
<section id="challenge">
<h2>The Challenge</h2>
<p><u>Background</u>: Each year, the Cadet Wing (CW) assesses the over 4000 cadets at the US Air Force Academy. Two components of this&mdash;the personal appearance inspection and cadet interview&mdash;had been carried out on paper or used a survey tool to log interview responses. </p>

<p><u>Goal</u>: Cut down on the number of hours required to collect and analyze the data by developing a user-friendly, digital tool that will store the data in one location.</p>

<p><u>My Role</u>: Product designer and trainer. I designed the product from beginning to end, conducting usability testing along the way.</p> 
</section>

<section id="process">
<h2>The Iterative Design Process</h2>
<h3>User Research</h3>
<p>The CW conducted the user research on the assessors and data analysts. From their research, I formed the following insights
<ul>
  <li>For the assessors:</li>
    <ul>
      <li> They're time crunched: they do not have much time to learn a new tool or tools, and assessments need to be completed efficiently.</li>
      <li> They exhibit a spectrum of comfort and abilities with technology: the product needs to be easy to use and intuitive.</li>
      <li> They feel pressure to have their squadron receive high marks.</li>
    </ul>
  <li>For the data analysts:</li>
    <ul>
      <li> They wanted granularity in the data: they needed to be able to track comments, which interview questions were asked, and how many points were given for each category.</li>
      <li> They were concerned about the integrity of the data.</li>
      <li> They needed one-stop access to data: data for both assessments needs to be stored in one location.</li>
      <li> They had more familiarity and comfort with technology than most assessors.</li>
    </ul>
</ul>
</p>

<p>I added a third user to consider: the cadet. For cadet's, the main need was the ability to see their scores after the assessment was over.</p>

<h3>UX Methods</h3>
The main UX skills used in this project were:
<ul>
  <li><u>Defining the Problem</u>: We were initially approached with <q>We want to use the LMS for cadet assessment</q>. Through careful questioning and listening I was able to craft concrete goals for the project and identify constraints.</li>
 <li><u>Ideation and Journey Mapping</u>: While ideating for possible solutions, I created journey maps for viable solutions, such as the one below.
<br><img class="img-responsive" src="/img/CW-Rubric-Journey.jpg" alt="The user journey for either rubric"><br>
These journey maps helped explain the pro's and con's of the two viable solutions that came out of the ideation phase.</li>
<li><u>Prototyping</u>: Because we have limited control over the user interface, I went straight to prototyping possible solutions in the LMS. These were high fidelty prototypes that let the stakeholders see what the final product would look like&mdash;and let me focus on design the simplest and cleanest interactions with the LMS.</li>
<li><u>Usability Testing and Iterating</u>: Given the short lifespan of this project, I opted to adapt an agile framework. I could quickly produce mockups of ideas and conduct usability testing at regularly scheduled meetings, then ideate on the mockup using the feedback from testing.</li>
</ul>


<h3>Interaction Design Decisions</h3>
<p>One of the initial decision was whether the LMS could be leveraged to achieve the goal. After defining the problem, it was clear that it could. We then had to make the following decisions during the design and development of the rubrics.</p>

<h4>Choosing the Best Tool</h4>
<p>I quickly concluded that the LMS had two tools that could be used to achieve our goal: the tests tool and the rubric tool. I created some mockups of each to showcase the functionality.<br>
<img class="img-responsive" src="/img/EarlyMockup-Interviewiew-Test.PNG" alt="Early mockup of the interview as a test."><br>
<img class="img-responsive" src="/img/EarlyMockup-PAI-Test.PNG" alt="Early mockup of the appearance inspection as a test."><br>
<img class="img-responsive" src="/img/02-Rubric1.PNG" alt="Early mockup of the appearance inspection as a rubric."></p>

<p>We settled on the rubric for the following reasons:
  <ol>
  <li> Assessors would only need to learn and use on tool instead of two.</li>
  <li> The workflow for the assessor was simplified: the best option for the test tool would involve multiple accounts and thereby slow down the assessment process, whereas the rubric tool required a single sign-on.</li>
  <li> Data retrieval with a rubric would be much cleaner than with the test tool.</li>
  <li> It would be easy to track changes in the data, thus helping ensure its integrity.</li>
  <li> The ability to provide written feedback in addition to a numerical score.</li>
  <li> The ability to track which interview questions were asked (since the interviewer was allowed to ask one of several questions per category).</li>
</ol>  

<h4>Grouping the Cadets</h4>
<p>To be used, rubrics have to be housed within a course in the LMS. So, we had to decide how many courses to create. The two main considerations for making this decision were:
  <ol>
    <li> The tradeoff: At one extreme, we could make data collection easy by using only one course&mdash;but it would have been much more cumbersome for the assessors who would need to wade through lots more students and face a slower load time. At the other extreme, we could make life easier for the assessors by creating one course per squadron (i.e. 40 courses total), since assessments were done (mostly) by squadron leaders</li>
    <li> The roles: assessments were <i>mostly</i> done by squadron leaders, but some cadets also were assessors. This created a problem because a user can only have one role in a course, essentially ruling out the one course option as well as the 40 course option.</li>
  </ol>
Ultimately, we settled on one course per class year, or four courses total. With four courses we could accommodate the double role that some cadets had. While each course would have over 1,000 cadets in it, the load times would still be quick and we could design filters to help assessors find cadets quickly.</p>
  
<h4>Accessing the Rubrics</h4>
<p>The appearance assessment had different rubrics for the two uniforms a cadet could wear. We had to decide when an assessor would choose the appropriate rubric: after launching the rubric window or before.</p>

<p>Launching the rubric window then choosing the rubric would keep the data in one column, but would make it easy to mistakenly fill in the wrong rubric: seeing <q>1 of 2</q> in the rubric window might make an assessor believe they had two rubrics to fill in instead of one.<br> <!--Maybe carousel the images?!?!-->
<img class="img-responsive" src="/img/01-RubricSelection.PNG" alt="Choosing between the two rubrics"><br>
<img class="img-responsive" src="/img/02-Rubric-VerticalFlow.png" alt="The first and second rubric options"></p>

<p>We chose to have the assessor choose the rubric before launching the rubric window. This would store the data in two columns in a spreadsheet, but essentially eliminated the possibility of filling in two rubrics. Fewer errors was worth the extra column.
  <br><img class="img-responsive" src="/img/03-Rubric.PNG" alt="The chosen rubric, with text box"><br>
(A quick note about the user interface: the LMS does not let you change the number of columns for certain rows, hence the “N/A DO NOT USE THIS BOX” boxes. If I had my druthers, I would have changed the number of options available in those rows. Also note the ability to add written feedback about a given mark.)</p>

<h4>Assigning Points</h4>
<p>I also had to make sure the math worked out as needed. For the interview the first few rows of the rubric had no points, since they contained the questions the interviewer asked the cadets and were not worth any points. To accomplish this I leveraged the weighting function of the rubric, assigning a 0% weight to those rows and non-zero weight to the rest (pretty neat, right?&mdash;which questions were asked were tracked by selecting the appropriate question within the rubric, while the rest of the rows were used for evaluating the cadet’s responses).
<br><img class="img-responsive" src="/img/04-Interview-Headers.PNG" alt="The interview rubric, with the categories in each cell."></p>
  
<p>For the uniform inspection rubric, there were two ways the rubric could calculate points: each category could be seen as an opportunity to gain points (i.e. a cadet started with a low score and worked his or her way to a higher score), or an opportunity to lose points (i.e. they started with a high score and lost points for infractions).
<br>
<img class="img-responsive" src="/img/06-Schema-Small1.PNG" alt="Point schema: lose points.">
<img class="img-responsive" src="/img/06-Schema-Small2.PNG" alt="Point schema: earn points.">
<br>
Between these, the leadership chose the option where cadets are trying not to lose points.</p>

<h4>Descriptions for Categories</h4>
<p>Given the length of the rubric, the final consideration for rubrics was whether to include the verbal description, as well as the level of achievement, of their assessment in each cell (look below the percentage in each cell in the following screenshots):
<br><img class="img-responsive" src="/img/04-Interview-Comparison.png" alt="The top rubric doesn't have the categories in cell whereas the bottom does."><br>
Since the rubrics would require scrolling to reach the bottom, I advised, and the leadership approved of, including the verbal description in each box so the assessor didn’t have to remember which column corresponded to which description.

<h4>Assigning a Grade: An Sneak Peak Before Analyzing the Data</h4>
<p>Our final decision was how assessment grades were displayed in the course. Keeping the cadet in mind, I suggested adding a feature that would display a pass or fail grade along with the score received on each assessment once the inspection or interview rubric was completed and saved. The leadership liked the thought of providing a letter grade so the cadets knew whether they passed, so we added this feature.</p>
</section>

<section id="Results">
<h2>Results</h2>
<p><ol>
<li> Saved over 915 hours of annual work collecting and analyzing assessment data.
<li> Overwhelmingly positive feedback, including a letter of commendation from the Vice Commandant of Cadets noting how <q>Dr. Padgett’s innovative placement of the Personal Appearance Inspection (PAI) into Blackboard is a game changer for us</q>. </li>
<li> Over 200 assessors who are happy to use the rubrics for Cadet Assessment.</li>
<li> Full committment from the CW to adopt the LMS for all training and assessment programs.</li>
</ol></p>
</section>
