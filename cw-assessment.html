---
layout: static1
title: Saving Time With User-Centered Design
permalink: /cw-assessment
---
<h1>The Challenge</h1>
Each year, the Cadet Wing (CW) assesses the over 4000 cadets at the US Air Force Academy. 
Two components of this&ndash;the personal appearance inspection and cadet interview&ndash;had been carried out on paper or used a survey tool to log responses. 
This process required hundreds of hours of work to compile and analyze the data at the end of the assessment period, aside from the number of hours spent carrying out the inspections and interviews. 
With the recent acquisition of a Learning Management System (LMS), the CW wanted to utilize the LMS to save time and have all the data entered and stored in one location. 
They contacted our team, and I took the lead on this project, carrying out the design and development of the products, and providing training on how to use them. 
<br>
<h1>The Iterative Design Process</h1>
<p>At our kickoff meeting, the CW leadership stated their goals for this project and described how the inspection and interview took place.
  Given the nature of the project, I determined that iterative design was the best way to arrive the optimal products. 
  So, we agreed to meet at regular intervals so I could show them working prototypes and conduct usability testing.</p>

<p>Given my knowledge of the LMS, I developed initial prototypes using two different tools in the LMS. 
  During my second meeting, my goal was to come to a decision on which tool to use for each component of the assessment.
  To accomplish this, I asked questions to obtain their preferences on: 
<ul>
  <li> How data was put into the system; </li>
  <li> How and what data would be pulled out of the system; </li>
  <li> What sorts of workflows were ideal, acceptable, and unacceptable.</li>
</ul>
Their responses led us to conclude that the rubric tool was the best one to use for both the inspection (a straightforward use of a rubric) and the interview (a rather innovative use of the tool).
This decision hinged on four considerations:
<ol>
  <li> Assessors would only need to learn and use on tool instead of two.</li>
  <li> The workflow for the assessor was simplified: given their preferences for data entry, the workflow for the other tool I suggested as a possible solution would require using multiple accounts and thereby slow down the assessment process.</li>
  <li> Given their preferences for pulling data, the data retrieval with a rubric would be much cleaner than the other tool.</li>
  <li> The ability to provide written feedback in addition to a numerical score.</li>
  <li> The ability to track which interview questions were asked (since the interviewer was allowed to ask one of several questions per category).</li>
</ol>
</p>
<p> Our subsequent meetings were spent working out three components of the user experience: how the assessor would select the appropriate rubric; how each rubric would calculate points; and whether to include an automatically assigned pass/fail grade after the assessment and, if so, whether to color code it.</p>
 
<h2>Choosing the Right Rubric</h2>

  <h2>Assign Points</h2>

  <h2>Assigning a Grade</h2>

<h1>Outcome</h1>
<p>Once the design process was over, I trained over 80 people who would either use the rubrics or train others on how to use the rubrics. 
  The feedback we received at the end of the assessment period was overwhelmingly positive. 
  Among the positive feedback we received was a letter of commendation from the Vice Commandant of Cadets noting how <q>Dr. Padgettâ€™s innovative placement of the Personal Appearance Inspection (PAI) into Blackboard is a game changer for us</q>. 
  And the numbers do not lie about that: not only were the users happy with the experience provided by the rubrics, but overall these rubrics saved CW staff 915 hours of work (per assessment cycle)!</p>
