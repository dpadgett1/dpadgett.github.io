---
layout: static1
title: Saving Time With User-Centered Design
permalink: /cw-assessment
---
<h1>The Challenge</h1>
Each year, the Cadet Wing (CW) assesses the over 4000 cadets at the US Air Force Academy. 
Two components of this&mdash;the personal appearance inspection and cadet interview&mdash;had been carried out on paper or used a survey tool to log responses. 
This process required hundreds of hours of work to compile and analyze the data at the end of the assessment period, aside from the number of hours spent carrying out the inspections and interviews. 
With the recent acquisition of a Learning Management System (LMS), the CW wanted to utilize the LMS to save time and have all the data entered and stored in one location. 
They contacted our team, and I took the lead on this project, carrying out the design and development of the products, and providing training on how to use them. 
<br>
<h1>The Iterative Design Process</h1>
<p>At our kickoff meeting, the CW leadership stated their goals for this project and described how the inspection and interview took place.
  Given the nature of the project, I determined that iterative design was the best way to arrive the optimal products. 
  So, we agreed to meet at regular intervals so I could show them working prototypes and conduct usability testing in order to gather feedback to improve the design of the products.</p>

<p>During our initial meeting we also discussed who would be using the products.
  Because of time constraints on the project, the leadership provided demographics on the <b>assessors</b>:
  <ul>
    <li> Time crunched: not much time to learn a new tool or tools, assessments need to be completed efficiently.</li>
    <li> Spectrum of comfort and abilities with technology: the experience needs to be straightforward, the product needs to be easy to use and intuitive.</li>
    <li> Pressure to have their squadron receive high marks.</li>
</ul>
  From our interactions in meetings, I was able to put together a profile for those who would be handling the data after it was collected, the <b>leadership</b>.
  <ul>
    <li> Granularity of data: need to track comments, which interview questions are asked, points given for each category.</li>
    <li> Concerned with the integrity of the data.</li>
    <li> Access to data: data for both assessments needs to be stored in one location.</li>
    <li> More familiarity and comfort with technology than most assessors.</li>
</ul>
</p>

<p>With this information about the users, I developed initial prototypes using two different tools in the LMS. 
  During my second meeting, my goal was to come to a decision on which tool to use for each component of the assessment.
  At this meeting I demonstrated the capabilities of each tool, noting how they would impact the user experience with respect to the aforementioned demographics.</p>
  
By the end of this meeting we concluded that the rubric tool was the best one to use for both the appearance inspection (a straightforward use of a rubric) and the interview (a rather innovative use of the tool).
This decision hinged on four considerations:
<ol>
  <li> Assessors would only need to learn and use on tool instead of two.</li>
  <li> The workflow for the assessor was simplified: given their preferences for data entry, the workflow for the other tool I suggested as a possible solution would require using multiple accounts and thereby slow down the assessment process.</li>
  <li> Given their preferences for pulling data, the data retrieval with a rubric would be much cleaner than the other tool.</li>
  <li> It would be easy to track changes in the data, thus helping ensure its integrity.</li>
  <li> The ability to provide written feedback in addition to a numerical score.</li>
  <li> The ability to track which interview questions were asked (since the interviewer was allowed to ask one of several questions per category).</li>
</ol>
</p>
<p> Our subsequent meetings were spent working out three components of the user experience: how the assessor would select the appropriate rubric; how each rubric would calculate points; and whether to include an automatically assigned pass/fail grade after the assessment and, if so, whether to color code it.</p>
 
<h2>Choosing the Right Rubric</h2>
<p>For getting the right rubric, we had to decide:
  <ul>
    <li> The best way to group the cadets (create one course in the LMS with all of the cadets, create one course per class year, create one class per squadron, etc).</li>
    <li> The best way to access the rubrics within a course.</li>
</ul>
Ultimately, it made the most sense to have one course per class year. 
The user's journey for completing each assessment would look like this:
<br><img src="/img/CW-Rubric-Journey.jpg" alt="The user journey for either rubric" style="height:350px"><br></p>

<p>Within the LMS, a rubric is associated with a column in the course’s gradebook. 
  Any column can have more than one rubric associated with it. 
  With the personal appearance inspection, there were two different rubrics, depending on the choice of uniform. 
  So, one way to place the rubrics in the course was to associate both with one column, which would look like this:<br>
  <img src="/img/01-RubricSelection.PNG" alt="Choosing between the two rubrics" style="height:150px"></p>

<p> Once they made their choice, they would see the rubric they selected, as well as a choice to use the other rubric (see top-right).
  After filling out the selected rubric they could submit the score or move onto the next one and submit:<br>
  <img src="/img/02-Rubric-VerticalFlow.png" alt="The first and second rubric options" style="height:700px"><br>
One downside of this approach is that it would be easy to mistakenly fill in the wrong rubric: seeing <q>1 of 2</q> might make an assessor believe they had two rubrics to fill in instead of one.</p>

<p>The second option, which was what the leadership chose, was to have two columns, one for one uniform choice and a second for the other.
  The rubric looked the same but no longer had <q>1 of 2</q> and the option to choose a second rubric once it was open:
  <br><img src="/img/03-Rubric.PNG" alt="The chosen rubric, with text box" style="height:350px"><br>
  (A quick note about the user interface: the LMS does not let you change the number of columns for certain rows, hence the “N/A DO NOT USE THIS BOX” boxes. 
  If I had my druthers, I would have changed the number of options available in those rows. Also note the ability to add written feedback about a given mark.)</p>

  <h2>Assigning Points</h2>
<p>For the interview I had to make sure that the first few rows of the rubric had no weight, since they contained the questions the interviewer asked the cadets (pretty neat, right?&mdash;which questions were asked were tracked by selecting the appropriate question within the rubric, while the rest of the rows were used for evaluating the cadet’s responses).
<br><img src="/img/04-Interview-Headers.PNG" alt="The interview rubric, with the categories in each cell." style="height:350px"></p>

<p>Given the length of the rubric, the other consideration for the interview and personal appearance inspection was whether to include the verbal description of their assessment in each cell:
<br><img src="/img/04-Interview-Comparison.png" alt="The top rubric doesn't have the categories in cell whereas the bottom does." style="height:450px"><br>
Since the rubrics would require scrolling to reach the bottom, I advised including the verbal description in each box so the assessor didn’t have to remember which column corresponded to which description.</p>

<p>For the uniform inspection rubric, there were two ways the rubric could calculate points: each category could be seen as an opportunity to gain points (i.e. a cadet started with a low score and worked his or her way to a higher score), or an opportunity to lose points (i.e. they started with a high score and lost points for infractions). 
  Working through the choice of point-scoring schemas provided an opportunity to consult the leadership on a heretofore unconsidered user: the cadet. 
  While the assessors and staff would be the primary users of these rubrics, the cadets would eventually see the rubrics, and it was important to think about how they would view the point distribution: should they view the inspection as an opportunity to prove themselves or to not make a mistake?</p>
<br>
<img src="/img/06-Schema-Small1.PNG" alt="Point schema: lose points." style="height:140px">
<img src="/img/06-Schema-Small2.PNG" alt="Point schema: earn points." style="height:140px">

<h2>Assigning a Grade</h2>
<p>One feature of the LMS is that you can create a grading schema to convert a numerical score to a letter grade. Again keeping the cadet in mind, I presented this option to the CW leadership as something we could add in. Once the inspection or interview rubric was completed and saved, the LMS would show a pass or fail grade along with the score received on each assessment. The leadership liked the thought of providing a letter grade so the cadets knew whether they passed, so we implemented a grading schema based on their requirements into the final solution.</p>

<h1>Outcome</h1>
<p>Once the design process was over, I trained over 80 people who would use the rubrics and train 120 others on how to use the rubrics. 
  The feedback we received at the end of the assessment period was overwhelmingly positive. 
  Among the positive feedback we received was a letter of commendation from the Vice Commandant of Cadets noting how <q>Dr. Padgett’s innovative placement of the Personal Appearance Inspection (PAI) into Blackboard is a game changer for us</q>. 
  And the numbers do not lie about that: not only were the users happy with the experience provided by the rubrics, but overall these rubrics saved CW staff 915 hours of work (per assessment cycle)!</p>
